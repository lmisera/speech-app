{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "757b82d4-82a0-464f-8bda-f5a7b695a9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lucas\\onedrive\\python\\venv\\lib\\site-packages (2.1.4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Lucas\\OneDrive\\python\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lucas\\onedrive\\python\\venv\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucas\\onedrive\\python\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucas\\onedrive\\python\\venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\lucas\\onedrive\\python\\venv\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucas\\onedrive\\python\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install PyMuPDF\n",
    "!pip install pandas\n",
    "import fitz\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "directory = 'PDFs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52801764-40cf-40ee-ac10-5f2bbf0841e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PDFs\\\\Akron-Districts-Speech-Ballots-24.pdf', 'PDFs\\\\Canton-McKinley-Speech-Ballots.pdf', 'PDFs\\\\Copley-Part-2-Speech-Ballots.pdf', 'PDFs\\\\Copley-Speech-Ballots.pdf', 'PDFs\\\\Eastern-Ohio-Districts-Speech-Ballots-24.pdf', 'PDFs\\\\Highland-Speech-Ballots.pdf', 'PDFs\\\\Hoover-Speech-ballots-24.pdf', 'PDFs\\\\Northwest-Speech-Ballots.pdf', 'PDFs\\\\Norton-Speech-Ballots-24.pdf', 'PDFs\\\\Perry-Speech-Ballots.pdf', 'PDFs\\\\Stow-Hudson-speech-ballots.pdf', 'PDFs\\\\Tusky-Valley-Speech-Ballots.pdf', 'PDFs\\\\University-Speech-Ballots-24.pdf', 'PDFs\\\\Wadsworth-Speech-Ballots.pdf', 'PDFs\\\\Wooster-speech-24.pdf', 'PDFs\\\\Wooster-Speech-Ballots.pdf']\n"
     ]
    }
   ],
   "source": [
    "#Import PDFs from folder\n",
    "pdf_list = []\n",
    "\n",
    "for files in os.listdir(directory):\n",
    "    if \"speech\" in files:\n",
    "        f = os.path.join(directory,files)\n",
    "        pdf_list.append(f)\n",
    "    elif \"Speech\" in files:\n",
    "        f = os.path.join(directory,files)\n",
    "        pdf_list.append(f)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(pdf_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b616b0ca-8ec0-4faa-b7f8-d3b648e9e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments:1348\n",
      "Number of rounds: 1348\n",
      "Number of ranks: 1348\n",
      "Number of events: 1348\n",
      "Number of students: 1348\n",
      "Number of judges: 1348\n",
      "                                               Event  \\\n",
      "0                     OSDA Akron District Tournament   \n",
      "1                     OSDA Akron District Tournament   \n",
      "2                     OSDA Akron District Tournament   \n",
      "3                     OSDA Akron District Tournament   \n",
      "4                     OSDA Akron District Tournament   \n",
      "...                                              ...   \n",
      "1343  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1344  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1345  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1346  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1347  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "\n",
      "                              Round  \\\n",
      "0     Dramatic Interpretation Rd. 1   \n",
      "1     Dramatic Interpretation Rd. 1   \n",
      "2     Dramatic Interpretation Rd. 2   \n",
      "3     Dramatic Interpretation Rd. 2   \n",
      "4     Dramatic Interpretation Rd. 3   \n",
      "...                             ...   \n",
      "1343     Congressional Debate Rd. 1   \n",
      "1344     Congressional Debate Rd. 1   \n",
      "1345     Congressional Debate Rd. 2   \n",
      "1346     Congressional Debate Rd. 2   \n",
      "1347     Congressional Debate Rd. 2   \n",
      "\n",
      "                                              Judge               Name  \\\n",
      "0              X7 Mike Chadsey (Independent Judges)  508 Alistar Mocek   \n",
      "1                         L3 Manohar Rajan (Copley)  508 Alistar Mocek   \n",
      "2            X9 Stacey Spinder (Independent Judges)  508 Alistar Mocek   \n",
      "3                        D5 Bola Majekodunmi (CVCA)  508 Alistar Mocek   \n",
      "4                 B4 Adrienne Cvetkovic (Revere HS)  508 Alistar Mocek   \n",
      "...                                             ...                ...   \n",
      "1343      Z4 Avery Fisher (Canton Central Catholic)   89 Jackson Moore   \n",
      "1344  K6 Jair Carrasquero (Centerville High School)   89 Jackson Moore   \n",
      "1345                     AA11 Maria Loznianu (Stow)   89 Jackson Moore   \n",
      "1346     Z2 Ronnie Perdue (Canton Central Catholic)   89 Jackson Moore   \n",
      "1347      BI3 Matthew Powell (John F. Kennedy H.S.)   89 Jackson Moore   \n",
      "\n",
      "                                               Feedback  Rank  \\\n",
      "0     508 was the fourth best speaker in the round. ...     4   \n",
      "1     508 - Alistar Mocek (Just Checking) is ranked ...     4   \n",
      "2     Not sure if its in the character you are portr...     3   \n",
      "3     Identifier: Just Checking\\nA very engaging pre...     3   \n",
      "4     You are easily heard and know your piece well....     3   \n",
      "...                                                 ...   ...   \n",
      "1343                    No comments provided by judge\\n     7   \n",
      "1344                    No comments provided by judge\\n     2   \n",
      "1345                    No comments provided by judge\\n     2   \n",
      "1346                    No comments provided by judge\\n     8   \n",
      "1347                    No comments provided by judge\\n     7   \n",
      "\n",
      "                  Date  \n",
      "0         Feb. 3, 2024  \n",
      "1         Feb. 3, 2024  \n",
      "2         Feb. 3, 2024  \n",
      "3         Feb. 3, 2024  \n",
      "4         Feb. 3, 2024  \n",
      "...                ...  \n",
      "1343  Jan. 12-13, 2024  \n",
      "1344  Jan. 12-13, 2024  \n",
      "1345  Jan. 12-13, 2024  \n",
      "1346  Jan. 12-13, 2024  \n",
      "1347  Jan. 12-13, 2024  \n",
      "\n",
      "[1348 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#USE THIS SECTINO AS THE NEW REGEX\n",
    "#FIGURE OUT HOW TO GET TEXT FROM MATCH\n",
    "\n",
    "perm_comments = []\n",
    "perm_ranklist = []\n",
    "perm_eventlist = []\n",
    "perm_rounds = []\n",
    "perm_studentlist = []\n",
    "perm_judgelist = []\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for i in pdf_list:\n",
    "    doc = fitz.open(i)\n",
    "    for page in doc:\n",
    "        all_text+=page.get_text()\n",
    "    doc.close()\n",
    "\n",
    "#print(all_text)\n",
    "\n",
    "#Comments\n",
    "comment_pattern = re.compile(r\"(?s)(?<=Comments\\n)(.*?)(?=NC Rotary|Canton McKinley|Eastern Ohio Main|OSDA Akron District|Norton Speech & Debate|University School Tournament|Copley Invitational|Highland Veteran's Day|Inaugural Northwest Rotary Holiday Classic|63rd Annual Perry Varsity Speech|The Suzanne Theisen|The Tusky Valley Jeffery Worrell Memorial|Wadsworth High School Grizzly Classic|Wooster's Sharen B Althoff Rotary Invitational|Wooster's Sharen B Althoff Rotary Invitational|\\Z)\", re.DOTALL)\n",
    "comments = comment_pattern.findall(all_text)\n",
    "print(\"Number of comments:\" + str(len(comments))) #120\n",
    "#print(comments)\n",
    "\n",
    "#Rounds\n",
    "regex = r\"^.*(?=\\sSect.\\s[a-z])\"\n",
    "matches = re.finditer(regex, all_text, re.MULTILINE | re.IGNORECASE)\n",
    "rounds = []\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    rounds.append(match.group(0))\n",
    "print(\"Number of rounds: \" + str(len(rounds)))\n",
    "\n",
    "#Ranks\n",
    "regex = r\"(?<=\\nRank:\\s)[0-9]*(?=\\nTime)\"\n",
    "matches = re.finditer(regex, all_text, re.MULTILINE | re.IGNORECASE)\n",
    "ranklist = []\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    ranklist.append(match.group(0))\n",
    "print(\"Number of ranks: \" + str(len(ranklist)))\n",
    "\n",
    "#Events\n",
    "regex = r\"^(.*)(NC Rotary|Canton McKinley|Eastern Ohio Main|OSDA Akron District|Norton Speech & Debate|University School Tournament|Copley Invitational|Highland Veteran's Day|Inaugural Northwest Rotary Holiday Classic|63rd Annual Perry Varsity Speech|The Suzanne Theisen|The Tusky Valley Jeffery Worrell Memorial|Wadsworth High School Grizzly Classic|Wooster's Sharen B Althoff Rotary Invitational|Wooster's Sharen B Althoff Rotary Invitational).*(202[0-9]).*$\"\n",
    "matches = re.finditer(regex, all_text, re.MULTILINE | re.IGNORECASE)\n",
    "eventlist=[]\n",
    "eventlist_final = []\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    eventlist.append(match.group(0))\n",
    "for i in eventlist:\n",
    "    eventlist_final.append(i)\n",
    "print(\"Number of events: \" + str(len(eventlist_final)))\n",
    "\n",
    "\n",
    "regex = r\"(?<=Entry:\\s)(\\d+)\\s(.*)\"\n",
    "matches = re.finditer(regex, all_text, re.MULTILINE | re.IGNORECASE)\n",
    "studentlist = []\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    studentlist.append(match.group(0))\n",
    "print(\"Number of students: \" + str(len(studentlist)))\n",
    "\n",
    "\n",
    "regex = r\"(?<=Judge:\\s).*$\"\n",
    "matches = re.finditer(regex, all_text, re.MULTILINE | re.IGNORECASE)\n",
    "judgelist = []\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    judgelist.append(match.group(0))\n",
    "print(\"Number of judges: \" + str(len(studentlist)))\n",
    "\n",
    "\n",
    "\n",
    "#Build dataframe\n",
    "df = pd.DataFrame({'Event':eventlist_final,'Round':rounds,'Judge':judgelist,'Name':studentlist,'Feedback':comments,'Rank':ranklist})\n",
    "\n",
    "#Replace missing ranks with 6s\n",
    "df['Rank'].replace('','6',inplace=True)\n",
    "df['Rank'] = df['Rank'].astype(int)\n",
    "\n",
    "df[['Event','Date']] = df['Event'].str.split(' - ', n=1,expand=True)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58fb69f5-ebdc-48a2-b0ad-7d746522939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['508 Alistar Mocek', '126 Donnell Maxwell', '506 Ella Embry', '410 Emily Chikosky', '608 Hector Studer', '913 Hector Studer', '210 Jenny Baniya', '308 Jenny Ostlund', '411 Kendall Garlesky', '313 Kennedie Ward', '514 Kiera Sayre', '710 Malak Zaman', '209 Michelle Clark', '711 Perrin Leasure', '712 Rachel Senderoff', '615 River Zellner', '114 Rogan Allen', '607 Ryder Kehres', '609 Tyler Jones', '309 Ziah Pittman ', '811 Otis Schoenberg and Cecelia Lattime', '514 Ella Embry', '417 Emily Chikosky', '616 Hector Studer', '810 Lauren Garfield and Penelope Covey', '719 Malak Zaman', '513 Penelope Covey', '718 Perrin Leasure', '113 Rogan Allen', '617 Ryder Kehres', '814 Otis Schoenberg and Cecelia Lattime', '515 Ella Embry', '421 Emily Chikosky', '113 Griffin Moore', '620 Hector Studer', '923 Jenny Ostlund', '723 Malak Zaman', '713 Perrin Leasure', '112 Rogan Allen', '619 Ryder Kehres', '124 Andrew Faluhelyi', '627 Hector Studer', '816 Penelope Covey and Lauren Garfield', '519 Penelope Covey', '529 Ella Embry', '530 Emily Chikosky', '632 Emily Chikosky', '137 Griffin Moore', '630 Hector Studer', '935 Hector Studer', '353 Jackson Moore', '532 Jenny Baniya', '336 Jenny Baniya', '820 Penelope Covey and Lauren Garfield', '729 Malak Zaman', '219 Michelle Clark', '821 Cecelia Lattime and Otis Schoenberg', '531 Penelope Covey', '730 Perrin Leasure', '731 Rachel Senderoff', '631 Rachel Senderoff', '138 Rogan Allen', '629 Ryder Kehres', '107 Andrew Faluhelyi', '801 Otis Schoenberg and Cecelia Lattime', '504 Ella Embry', '401 Emily Chikosky', '606 Hector Studer', '904 Jenny Ostlund', '705 Malak Zaman', '505 Penelope Covey', '903 Rachel Senderoff', '605 Ryder Kehres', '827 Otis Schoenberg and Cecelia Lattime', '543 Ella Embry', '449 Emily Chikosky', '641 Hector Studer', '951 Hector Studer', '450 Kendall Garlesky', '150 Lukas Prominski', '151 Rogan Allen', '640 Ryder Kehres', '528 Ella Embry', '422 Emily Chikosky', '733 Perrin Leasure', '116 Rogan Allen', '627 Ryder Kehres', '139 Andrew Klush', '501 Ella Embry', '103 Griffin Moore', '201 Jenny Baniya', '402 Kendall Garlesky', '202 Michelle Clark', '502 Penelope Covey', '802 Lauren Garfield and Penelope Covey', '701 Rachel Senderoff', '102 Rogan Allen', '601 Ryder Kehres', '167 Andrew Faluhelyi', '826 Otis Schoenberg and Cecelia Lattime', '540 Ella Embry', '643 Hector Studer', '739 Malak Zaman', '825 Lauren Garfield and Penelope Covey', '738 Perrin Leasure', '642 Ryder Kehres', '452 Salvatore Peak', '802 Otis Schoenberg and Cecelia Lattime', '405 Emily Chikosky', '801 Penelope Covey and Lauren Garfield', '503 Nicole Wakefield', '902 Rachel Senderoff', '403 Sadali Costa', '404 Salvatore Peak', '812 Otis Schoenberg and Cecelia Lattime', '428 Emily Chikosky', '119 Griffin Moore', '628 Hector Studer', '930 Jenny Ostlund', '811 Penelope Covey and Lauren Garfield', '731 Malak Zaman', '527 Penelope Covey', '929 Rachel Senderoff', '133 Rogan Allen', '153 Andrew Faluhelyi', '527 Ella Embry', '429 Emily Chikosky', '624 Hector Studer', '926 Hector Studer', '428 Kendall Garlesky', '733 Malak Zaman', '204 Michelle Clark', '816 Cecelia Lattime and Otis Schoenberg', '734 Perrin Leasure', '443 Perrin Leasure', '735 Rachel Senderoff', '120 Rogan Allen', '451 Emily Chikosky', '648 Hector Studer', '962 Hector Studer', '829 Penelope Covey and Lauren Garfield', '750 Malak Zaman', '255 Michelle Clark', '830 Cecelia Lattime and Otis Schoenberg', '553 Penelope Covey', '751 Perrin Leasure', '449 Perrin Leasure', '749 Rachel Senderoff', '185 Rogan Allen', '647 Ryder Kehres']\n",
      "['Dramatic Interpretation Rd. 1' 'Dramatic Interpretation Rd. 2'\n",
      " 'Dramatic Interpretation Rd. 3'\n",
      " 'United States Extemporaneous Speaking Rd. 1'\n",
      " 'United States Extemporaneous Speaking Rd. 2'\n",
      " 'United States Extemporaneous Speaking Rd. 3' 'Declamation Rd. 1'\n",
      " 'Declamation Rd. 2' 'Declamation Rd. 3' 'Humorous Interpretation Rd. 1'\n",
      " 'Humorous Interpretation Rd. 2' 'Humorous Interpretation Rd. 3'\n",
      " 'Informative Speaking Rd. 1' 'Informative Speaking Rd. 2'\n",
      " 'Informative Speaking Rd. 3'\n",
      " 'International Extemporaneous Speaking Rd. 2'\n",
      " 'International Extemporaneous Speaking Rd. 3'\n",
      " 'International Extemporaneous Speaking Rd. 1' 'Original Oratory Rd. 1'\n",
      " 'Original Oratory Rd. 2' 'Original Oratory Rd. 3'\n",
      " 'Program Oral Interpretation Rd. 1' 'Program Oral Interpretation Rd. 2'\n",
      " 'Program Oral Interpretation Rd. 3' 'Duo Interpretation Rd. 1'\n",
      " 'Duo Interpretation Rd. 2' 'Duo Interpretation Rd. 3'\n",
      " 'Duo Interpretation Final' 'Dramatic Interpretation Final'\n",
      " 'United States Extemporaneous Speaking Rd. 4'\n",
      " 'Humorous Interpretation Rd. 4' 'Duo Interpretation Rd. 4'\n",
      " 'Dramatic Interpretation Rd. 4' 'Humorous Interpretation Semifinal'\n",
      " 'Informative Speaking Rd. 4' 'Duo Interpretation Semifinal'\n",
      " 'Program Oral Interpretation Rd. 4'\n",
      " 'Program Oral Interpretation Semifinal'\n",
      " 'International Extemporaneous Speaking Rd. 4'\n",
      " 'Dramatic Interpretation Semifinal' 'Declamation Rd. 4'\n",
      " 'Humorous Interpretation Final'\n",
      " 'United States Extemporaneous Speaking Final'\n",
      " 'Program Oral Interpretation Final' 'Declamation Semifinal']\n",
      "                                               Event  \\\n",
      "0                     OSDA Akron District Tournament   \n",
      "1                     OSDA Akron District Tournament   \n",
      "2                     OSDA Akron District Tournament   \n",
      "3                     OSDA Akron District Tournament   \n",
      "4                     OSDA Akron District Tournament   \n",
      "...                                              ...   \n",
      "1286  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1287  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1288  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1289  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "1290  Wooster's Sharen B Althoff Rotary Invitational   \n",
      "\n",
      "                                            Round               Judge  \\\n",
      "0                   Dramatic Interpretation Rd. 1        Mike Chadsey   \n",
      "1                   Dramatic Interpretation Rd. 1       Manohar Rajan   \n",
      "2                   Dramatic Interpretation Rd. 2      Stacey Spinder   \n",
      "3                   Dramatic Interpretation Rd. 2    Bola Majekodunmi   \n",
      "4                   Dramatic Interpretation Rd. 3  Adrienne Cvetkovic   \n",
      "...                                           ...                 ...   \n",
      "1286  United States Extemporaneous Speaking Rd. 2         Megan Ugrin   \n",
      "1287  United States Extemporaneous Speaking Rd. 3      Amrapali Gupta   \n",
      "1288                Humorous Interpretation Rd. 1        Kat Grilliot   \n",
      "1289                Humorous Interpretation Rd. 2         Paul Gonder   \n",
      "1290                Humorous Interpretation Rd. 3        Laurie Repko   \n",
      "\n",
      "               Name                                           Feedback  Rank  \\\n",
      "0     Alistar Mocek  508 was the fourth best speaker in the round. ...     4   \n",
      "1     Alistar Mocek  508 - Alistar Mocek (Just Checking) is ranked ...     4   \n",
      "2     Alistar Mocek  Not sure if its in the character you are portr...     3   \n",
      "3     Alistar Mocek  Identifier: Just Checking\\nA very engaging pre...     3   \n",
      "4     Alistar Mocek  You are easily heard and know your piece well....     3   \n",
      "...             ...                                                ...   ...   \n",
      "1286    Rogan Allen  Love the movie reference. Allowed me to paint ...     4   \n",
      "1287    Rogan Allen  Intro was very interesting with the example of...     5   \n",
      "1288   Ryder Kehres  Liar-Liar\\nGreat use of different voices for e...     2   \n",
      "1289   Ryder Kehres  nice articulation and pitch highlights the mat...     4   \n",
      "1290   Ryder Kehres  Love the lawyer/liar debate LOL\\nDad's charact...     3   \n",
      "\n",
      "                  Date                                 Processed Feedback  \\\n",
      "0         Feb. 3, 2024  [fourth, best, speaker, round, wondering, 's, ...   \n",
      "1         Feb. 3, 2024  [alistar, mocek, checking, ranked, nice, effor...   \n",
      "2         Feb. 3, 2024  [sure, character, portraying, eye, contact, au...   \n",
      "3         Feb. 3, 2024  [identifier, checking, engaging, presentation,...   \n",
      "4         Feb. 3, 2024  [easily, heard, know, piece, well, first, tran...   \n",
      "...                ...                                                ...   \n",
      "1286  Jan. 12-13, 2024  [love, movie, reference, allowed, paint, visua...   \n",
      "1287  Jan. 12-13, 2024  [intro, interesting, example, personal, experi...   \n",
      "1288  Jan. 12-13, 2024  [liar-liar, great, use, different, voice, char...   \n",
      "1289  Jan. 12-13, 2024  [nice, articulation, pitch, highlight, materia...   \n",
      "1290  Jan. 12-13, 2024  [love, lawyer/liar, debate, lol, dad, 's, char...   \n",
      "\n",
      "      blocking  gesture  character  story  diction  enunciation  eye  \\\n",
      "0            0        0          0      0        0            0    0   \n",
      "1            0        0          1      0        0            0    0   \n",
      "2            0        0          1      0        0            0    1   \n",
      "3            0        0          0      1        0            0    0   \n",
      "4            0        0          0      0        0            0    0   \n",
      "...        ...      ...        ...    ...      ...          ...  ...   \n",
      "1286         0        0          0      0        0            0    0   \n",
      "1287         0        0          0      0        0            0    0   \n",
      "1288         0        0          1      0        0            0    0   \n",
      "1289         0        1          1      0        0            0    0   \n",
      "1290         0        0          1      0        0            0    0   \n",
      "\n",
      "      professional  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "...            ...  \n",
      "1286             0  \n",
      "1287             0  \n",
      "1288             0  \n",
      "1289             0  \n",
      "1290             0  \n",
      "\n",
      "[817 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text: convert to lowercase, remove punctuation, stop words, and lemmatize\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Additional words to exclude\n",
    "    additional_exclusions = {'rank'}\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Exclude stopwords, punctuation, numbers, and additional exclusions\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens \n",
    "                  if word not in stop_words \n",
    "                  and word not in string.punctuation \n",
    "                  and not word.isdigit()\n",
    "                  and word not in additional_exclusions]\n",
    "    return lemmatized\n",
    "\n",
    "df['Processed Feedback'] = df['Feedback'].apply(preprocess_text)\n",
    "\n",
    "word_counts = Counter(word for feedback in df['Processed Feedback'] for word in feedback)\n",
    "\n",
    "#Remove words that appear in every feedback\n",
    "unique_word_counts = {word: count for word, count in word_counts.items() if count<len(df)}\n",
    "\n",
    "word_counts = Counter(unique_word_counts)\n",
    "\n",
    "most_common = [word for word, count in word_counts.most_common(100)]\n",
    "\n",
    "#Choose words to care about\n",
    "selectedwords = ['blocking','gesture','character','story','diction','enunciation','eye','professional']\n",
    "\n",
    "# Create columns for each common word\n",
    "for word in selectedwords:\n",
    "    df[word] = df['Processed Feedback'].apply(lambda feedback: 1 if word in feedback else 0)\n",
    "\n",
    "#drop columns if...\n",
    "df = df[~df['Feedback'].str.contains(\"No comments provided by judge\")]\n",
    "\n",
    "#Clean up team names...\n",
    "\n",
    "namelist = []\n",
    "for i in df['Name']:\n",
    "    if i not in namelist:\n",
    "        namelist.append(i)\n",
    "    else:\n",
    "        pass\n",
    "print(namelist)\n",
    "\n",
    "#Clean up columns\n",
    "pattern = r'[a-zA-Z]\\d+' #Find letters followed by numbers\n",
    "pattern2 = r'\\([^)]*\\)' #Get rid of school names\n",
    "pattern3 = r'^[a-zA-Z] ' #Get rid of weird leading letters\n",
    "pattern4 = r'\\d+'\n",
    "df['Judge'] = df['Judge'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df['Judge'] = df['Judge'].apply(lambda x: re.sub(pattern2, '', x)).str.strip()\n",
    "df['Judge'] = df['Judge'].apply(lambda x: re.sub(pattern3, '', x))\n",
    "df['Judge'] = df['Judge'].str.strip()\n",
    "df['Judge'] = df['Judge'].str.lstrip()\n",
    "df['Name'] = df['Name'].str.strip()\n",
    "df['Name'] = df['Name'].apply(lambda x: re.sub(pattern4, '', x))\n",
    "df['Name'] = df['Name'].apply(lambda x: re.sub(r'^\\s+','',x)) #Remove leading spaces\n",
    "df['Name'] = df['Name'].apply(lambda x: re.sub(r'\\s+$','',x)) #Remove leading spaces\n",
    "\n",
    "#Clean up team names\n",
    "df['Name'] = np.where(df['Name']==\"Otis Schoenberg and Cecelia Lattime\",'Cecelia Lattime and Otis Schoenberg',df['Name'])\n",
    "df['Name'] = np.where(df['Name']==\"Lauren Garfield and Penelope Covey\",'Penelope Covey and Lauren Garfield',df['Name'])\n",
    "\n",
    "\n",
    "#Clean up event names\n",
    "df[\"Round\"] = df[\"Round\"].apply(lambda x: x.replace(\"Interp \", \"Interpretation \"))\n",
    "df[\"Round\"] = df[\"Round\"].apply(lambda x: x.replace(\"Humor Rd\", \"Humorous Interpretation Rd\"))\n",
    "df[\"Round\"] = df[\"Round\"].apply(lambda x: x.replace(\"Duo Rd\", \"Duo Interpretation Rd\"))\n",
    "df[\"Round\"] = df[\"Round\"].apply(lambda x: x.replace(\"Drama Rd\", \"Dramatic Interpretation Rd\"))\n",
    "df[\"Round\"] = df[\"Round\"].apply(lambda x: x.replace(\"US Extemp. Rd.\", \"United States Extemporaneous Speaking Rd.\"))\n",
    "print(df.Round.unique())\n",
    "\n",
    "# Now df has the additional columns for the common words\n",
    "df.to_excel(\"Speech_With_Data.xlsx\",index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "03bee9b6-3f2b-4bf3-8bc2-30cf24020851",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_rank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m streamlit_dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m streamlit_dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeedback\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRank:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m'\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      8\u001b[0m streamlit_dataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg Rank\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m streamlit_dataframe\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRank\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mavg_rank\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(streamlit_dataframe)\n\u001b[0;32m     15\u001b[0m st\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSum of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1s\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in Different Categories by Selected Name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_rank' is not defined"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "streamlit_dataframe = pd.read_excel(r'C:\\Users\\Lucas\\OneDrive\\python\\speech_and_debate\\Speech_With_Data.xlsx')\n",
    "streamlit_dataframe['Rank'] = streamlit_dataframe['Feedback'].str.extract(r'Rank:\\s*(\\d+)', expand=False).astype(float)\n",
    "streamlit_dataframe[\"Avg Rank\"] = streamlit_dataframe.groupby('Name')['Rank'].transform('mean')\n",
    "print(avg_rank)\n",
    "\n",
    "\n",
    "\n",
    "print(streamlit_dataframe)\n",
    "\n",
    "st.title('Sum of \"1s\" in Different Categories by Selected Name')\n",
    "\n",
    "\n",
    "\n",
    "# Dropdown to select a name\n",
    "selected_name = st.selectbox('Select a Name:', streamlit_dataframe['Name'].unique())\n",
    "\n",
    "# Filter data for the selected name\n",
    "filtered_data = streamlit_dataframe[streamlit_dataframe['Name'] == selected_name]\n",
    "feedback_data = streamlit_dataframe[streamlit_dataframe['Name'] == selected_name]\n",
    "\n",
    "#Get # of competitions\n",
    "number_of_competitions = filtered_data.shape[0]\n",
    "\n",
    "\n",
    "st.text(f'Average rank: {filtered_data[\"Avg Rank\"].iloc[0]}')\n",
    "st.text(f'Number of competitions: {number_of_competitions}')\n",
    "\n",
    "categories = ['blocking','gesture','character','story','diction','enunciation','eye','professional']\n",
    "\n",
    "# Sum the specified categories for the selected name\n",
    "sum_by_category = filtered_data[categories].sum()\n",
    "\n",
    "# Convert to DataFrame for easier display in Streamlit\n",
    "sum_by_category_df = sum_by_category.reset_index()\n",
    "sum_by_category_df.columns = ['Category', 'Sum of 1s']\n",
    "\n",
    "# Display the results in a table\n",
    "st.dataframe(sum_by_category_df)\n",
    "\n",
    "# Plotting the sums for each category by selected name\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(sum_by_category_df['Category'], sum_by_category_df['Sum of 1s'])\n",
    "ax.set_ylabel('Sum of 1s')\n",
    "ax.set_title(f'Sum of \"1s\" in Categories for {selected_name}')\n",
    "ax.set_xticklabels(sum_by_category_df['Category'], rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "st.pyplot(fig)\n",
    "\n",
    "#feedback_data = feedback_data.sort_values(by=['Event','Round'])\n",
    "\n",
    "for i in feedback_data['Feedback']:\n",
    "   st.text(str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca80d0-c028-4063-8fd1-3962e0eb9b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
